{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karolur/karolur/blob/main/IFT6135_A2025_HW1_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes: You will have to:\n",
        "  >1- Make a Copy of this notebook to edit it for your solutions;\n",
        "  >\n",
        "  >2- Upload on Gradescope: The `Colab Notebook edited with your solutions`, and a `pdf Report (Discussion questions)`."
      ],
      "metadata": {
        "id": "fyJ4Y-50zfCt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWoiYrUTh5q"
      },
      "source": [
        "# Question 1: MLP Implementation with Numpy\n",
        "\n",
        "In this exercise, we will explore the construction of a multi-layer perceptron (MLP) by coding it from the ground up. We will develop a deep neural network step-by-step, beginning with a single neuron, progressing to a layer, and ultimately building the entire network. We will then train this network and evaulate on an interesting dataset and try to understand the workings in more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJdB1Yv4T25Z"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ocuxjx05TKdn"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3idgvI3_T-dC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgDzUKUNUK_n"
      },
      "source": [
        "## Q 1.1 (a): Activation Functions\n",
        "\n",
        "In this question you will implement the softmax and ReLU activation functions and their derivatives. Both these functions help introduce non-linearity which gives neural network their capacity. We will implement these functions in the `ActivationFunction` class as static methods. You can access the method later using the following code:\n",
        "\n",
        "```\n",
        "# For softmax activation\n",
        "ActivationFunction.softmax(x)\n",
        "\n",
        "# For calucating the derivative of softmax function\n",
        "ActivationFunction.softmax_derivative(x)\n",
        "\n",
        "# For ReLU activation\n",
        "ActivationFunction.relu(x)\n",
        "\n",
        "# For calculating the derivative of ReLU function\n",
        "ActivationFunction.relu_derivative(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZLkQNPWULZe"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction:\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        \"\"\"\n",
        "        Compute the Softmax activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Softmax activation values\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the Softmax function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Derivative of Softmax function\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        \"\"\"\n",
        "        Compute the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: ReLU activation values\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Derivative of ReLU function\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77bPGi4UVCH"
      },
      "source": [
        "## Q 1.1 (b): Neuron Implementation\n",
        "\n",
        "Neuron is the fundamental building block of MLP. A neuron simulates a biological neuron which can be activated or fired when responding to a stimulus. The equation of a neuron is as follow:\n",
        "\n",
        "$$ f(x) = a(\\mathbf{w} \\cdot \\mathbf{x} + b) $$\n",
        "\n",
        "where inputs $\\mathbf{x}$ and weights $\\mathbf{w}$ are multidimensional vectors, bias $b$ is a scalar, $a(.)$ is an activation function.\n",
        "\n",
        "**Note:** For the whole exercise you will be receiving inputs of batch size of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaME5ZMeUYBz"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_size, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize a neuron with random weights and bias.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            activation_function (function): Activation function to use\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        # self.activation_function =\n",
        "        # self.weights =\n",
        "        # self.bias =\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\"\n",
        "        Compute the activation of the neuron.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            float: Activation value\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7nfeRx3UdPQ"
      },
      "source": [
        "## Q 1.1(c): Layer Implementation\n",
        "\n",
        "Now we will compose a layer which consists of multiple neurons. Complete initializing the layer and compute the forward pass by sending the input through each neuron of the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzrfOWC1UZ-m"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize a layer of neurons.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            output_size (int): Number of neurons in the layer\n",
        "            activation_function (function): Activation function to use\n",
        "        \"\"\"\n",
        "        ## Write your code here ##\n",
        "        # self.neurons =\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass through the layer.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Output of the layer\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeUxOv7kUh5z"
      },
      "source": [
        "## Q 1.1(d) + 2 (a) to (d): Neural Network Construction\n",
        "\n",
        "Now we have built all the building blocks required to build our neural network. Complete the implementations using the hints given in the code. There are additional functions that will be useful later. Do not modify these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeVTOZ_uUjrF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, hidden_dim, activation_function_hidden, activation_output):\n",
        "        \"\"\"\n",
        "        Initialize a neural network with specified layer sizes.\n",
        "\n",
        "        Args:\n",
        "            layer_sizes (list): List of integers representing the size of each layer\n",
        "            hiden_dim (int): Dimension of hidden layers\n",
        "            activation_function_hidden (function): Activation function to use for hidden layers\n",
        "            activation_output (function): Activation function to use for output layer\n",
        "        \"\"\"\n",
        "        self.layers = []\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.activation_function_hidden = activation_function_hidden\n",
        "        self.activation_output = activation_output\n",
        "        ## Write your code here ##\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass through the entire network.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Output of the network\n",
        "        \"\"\"\n",
        "        ## Write your code here ##\n",
        "        pass\n",
        "\n",
        "    def layer_backward(self, delta, layer_index, activations, zs):\n",
        "        \"\"\"\n",
        "        Perform backpropagation for a single layer.\n",
        "\n",
        "        Args:\n",
        "            delta (np.array): Gradient of the loss with respect to the output of the current layer\n",
        "            layer_index (int): Index of the current layer\n",
        "            activations (list): List of activations from the forward pass\n",
        "            zs (list): List of z values from the forward pass\n",
        "\n",
        "        Returns:\n",
        "            tuple: (delta for the previous layer, gradients for weights, gradients for biases)\n",
        "        \"\"\"\n",
        "        ## Write your code here for completing the ? ##\n",
        "        # layer = self.layers[?]\n",
        "        # z = zs[?]\n",
        "        # a_prev = activations[?]\n",
        "\n",
        "        if layer_index == len(self.layers) - 1:\n",
        "            ## Write your code here ##\n",
        "            # delta =\n",
        "            pass\n",
        "        else:\n",
        "            if self.activation_function == ActivationFunction.relu:\n",
        "                ## Write your code here ##\n",
        "                # delta =\n",
        "                pass\n",
        "            elif self.activation_function == ActivationFunction.softmax:\n",
        "                ## Write your code here ##\n",
        "                # delta =\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        ## Write your code here ##\n",
        "        # Hint: Make sure the shapes are properly handled\n",
        "        # gradient_w =\n",
        "        # gradient_b =\n",
        "        # delta_prev =\n",
        "\n",
        "        return delta_prev, gradient_w, gradient_b\n",
        "\n",
        "    def backward(self, x, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform backpropagation and update weights.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "            y (np.array): True labels\n",
        "            learning_rate (float): Learning rate for weight updates\n",
        "        \"\"\"\n",
        "        # Forward pass\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for layer in self.layers:\n",
        "            z = np.array([np.dot(neuron.weights, activations[-1]) + neuron.bias for neuron in layer.neurons])\n",
        "            zs.append(z)\n",
        "            activation = layer.forward(activations[-1])\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = activations[-1] - y\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            ## Write your code here ##\n",
        "            # delta, gradient_w, gradient_b =\n",
        "\n",
        "            # Update weights and biases\n",
        "            for j, neuron in enumerate(self.layers[i].neurons):\n",
        "                ## Write your code here ##\n",
        "                pass\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Training features\n",
        "            y (np.array): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            learning_rate (float): Learning rate for weight updates\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(X)):\n",
        "                self.backward(X[i], y[i], learning_rate)\n",
        "\n",
        "            if epoch % 50 == 0:\n",
        "                predictions = self.predict(X)\n",
        "                loss = compute_loss()\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def compute_loss():\n",
        "        \"Compute loss for model predictions and ground truth\"\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions for a set of input features.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Predictions\n",
        "        \"\"\"\n",
        "        return np.array([self.forward(x) for x in X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JicJ29ryUlDS"
      },
      "source": [
        "# Q 1.2(d): Training and Visualizing Neural Network\n",
        "\n",
        "In the rest of the exercise we will use the components we built earlier to train a neural network and visualise the loss curve. To make things simple we will be building neural networks with only one hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vO7ogsLUnP2"
      },
      "source": [
        "We will use the MNIST dataset for all our experiements which consists of 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gYj9nvIUqyr"
      },
      "outputs": [],
      "source": [
        "# Uncomment the below code to create the dataset. And comment it when submitting to gradescope\n",
        "\n",
        "# np.random.seed(42) # Do not change\n",
        "# For easy data loading and processing, we're using torch here. You're not allowed to use torch for Neural Net components though.\n",
        "def get_train_test_dataset():\n",
        "    import torch\n",
        "    from torchvision import datasets, transforms\n",
        "\n",
        "    # Define a transform to convert images to tensors and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Load the MNIST dataset\n",
        "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Take a subset of 1000 samples\n",
        "    X = mnist_dataset.data[:1000].float() / 255.0  # Normalize pixel values to [0,1]\n",
        "    y = mnist_dataset.targets[:1000]\n",
        "\n",
        "    # Creat test / test split\n",
        "\n",
        "    # to be completed\n",
        "    # print(\"Training set shape:\", )\n",
        "    # print(\"Test set shape :\", )\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's visualize a few samples"
      ],
      "metadata": {
        "id": "wSq1qSzQFPy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a few images\n",
        "\n",
        "def visualize_mnist_samples(X, y, num_samples=9):\n",
        "    \"\"\"Visualize MNIST sample images\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(min(num_samples, len(X))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        # Reshape from flattened to 28x28 for visualization\n",
        "        img = X[i].reshape(28, 28)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Label: {y[i]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('MNIST Sample Images')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "t1nM7oF6FNJd",
        "outputId": "1eff200a-38b2-4d91-951e-cdb6ba395f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTVzbcSUugW"
      },
      "source": [
        "Below you have been given the code to train your neural network. and visualize the decision boundry. Modify the hyperparameters to train and visualize the neural network and report your findings in the practical report. We will be testing our model on the training set. The `plot_decision_boundary` method takes the data created earlier and the neural network you trained to show the decision boundry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGqDsjU7Uu6b"
      },
      "outputs": [],
      "source": [
        "def train_network(hidden_layer_size, hidden_dim, epochs=10, activation_hidden=None, activation_output=None, learning_rate=0.001):\n",
        "    input_size, output_size = 28 * 28, 10\n",
        "    layers = [input_size, hidden_layer_size, output_size]\n",
        "\n",
        "    nn = NeuralNetwork(layers, hidden_dim, activation_hidden, activation_output)\n",
        "\n",
        "\n",
        "    # --- Prepare training data ---\n",
        "    X_train_flat =\n",
        "    y_train_np =\n",
        "\n",
        "    # One-hot encode labels\n",
        "    y_train_onehot =\n",
        "\n",
        "    # Train\n",
        "    train_loss = nn.train(X_train_flat, y_train_onehot, epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "    # --- Evaluate on training data ---\n",
        "    # preds_train =\n",
        "    # train_accuracy =\n",
        "\n",
        "    # --- Evaluate on test data ---\n",
        "    # X_test_flat =\n",
        "    # y_test_np =\n",
        "    # preds_test =\n",
        "    # test_accuracy =\n",
        "\n",
        "    return nn, train_loss, train_accuracy, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmQowEJgUySb"
      },
      "outputs": [],
      "source": [
        "# Write the code to train and visualize\n",
        "# Consider commenting this out when submitting to gradescope\n",
        "\n",
        "# hidden_layer_size =\n",
        "# hidden_dim =\n",
        "# epochs =\n",
        "# activation_hidden =\n",
        "# activation_output =\n",
        "# learning_rate =\n",
        "# nn, train_loss, train_accuracy, test_accuracy = train_network(hidden_layer_size, hidden_dim  epochs, activation_hidden, activation_output, learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q 1.3: Reporting and analysis\n",
        "Analyze your trained model and present findings clearly as asked in the instruction PDF."
      ],
      "metadata": {
        "id": "5mGVTWsJRpuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Train accuracy: \", train_accuracy)\n",
        "# print(\"Test accuracy: \", test_accuracy)"
      ],
      "metadata": {
        "id": "adETdDEQR0va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loss curve"
      ],
      "metadata": {
        "id": "jVHN2rRqSEOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RorfYPUfU2sI"
      },
      "outputs": [],
      "source": [
        "# Plot the training loss across 20 epochs\n",
        "\n",
        "def plot_loss_curve(train_loss):\n",
        "  # Write your code here\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer activation visualization"
      ],
      "metadata": {
        "id": "z7V5IzafMXJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Latent Space Visualization =====\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_penultimate_activations(nn, X_batch):\n",
        "    \"\"\"Get activations from the last hidden layer\"\"\"\n",
        "    # Process each sample individually\n",
        "    activations_list = []\n",
        "    for x in X_batch:\n",
        "        activations = x\n",
        "        for layer in nn.layers[:-1]:  # exclude final output layer\n",
        "            activations = layer.forward(activations)\n",
        "        activations_list.append(activations)\n",
        "    return np.array(activations_list)\n",
        "\n",
        "def visualize_activations(nn, X_test, y_test, digits=(3, 8), method='PCA'):\n",
        "    \"\"\"Visualize layer activations using PCA or t-SNE\"\"\"\n",
        "    # Get activations from penultimate layer\n",
        "    Z = get_penultimate_activations(nn, X_test)\n",
        "\n",
        "    # Apply dimensionality reduction\n",
        "    if method == 'PCA':\n",
        "        reducer = PCA(n_components=2)\n",
        "    else:  # t-SNE\n",
        "        reducer = TSNE(n_components=2, random_state=42)\n",
        "\n",
        "    Z_2d = reducer.fit_transform(Z)\n",
        "\n",
        "    # Plot visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot 2: Focus on specific digits\n",
        "    plt.subplot(1, 2, 2)\n",
        "    mask1 = y_test == digits[0]\n",
        "    mask2 = y_test == digits[1]\n",
        "\n",
        "    # write your code here\n",
        "\n",
        "    plt.title(f'Latent Space: Digits {digits[0]} vs {digits[1]}')\n",
        "\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return Z_2d"
      ],
      "metadata": {
        "id": "d1EaFVfDMb3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparemeter discussion\n",
        "You can try out changing hypermaters (leraning rate, hidden dim, num. of hidden layers) and report how that impact training loss, train/test accuracy"
      ],
      "metadata": {
        "id": "sjwJ0hiULHeV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cMyVm8aU42f"
      },
      "source": [
        "## Question 2: Convolutional Neural Network (using PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "QJCYgZBomFPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqSoGKwGYUYL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, random, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "validation_split = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 1: Implementing the CNN\n",
        "\n",
        "You will build the core components of the CNN, with **both** forward and backward passes.\n",
        "\n",
        "### (a) Fully Connected Layer\n",
        "Implement a linear layer:\n",
        "y = xW^T + b"
      ],
      "metadata": {
        "id": "PRCdv3WSmsa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywhrs-gTU_Aw"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias   = nn.Parameter(torch.Tensor(out_features)) if bias else None\n",
        "        self.reset_parameters()\n",
        "        self._cache_x = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Kaiming Uniform (fixed by assignment)\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Compute y = x W^T + b.\n",
        "        Hints:\n",
        "          • Use a batched matmul pattern that preserves leading batch dims.\n",
        "          • Add bias if present.\n",
        "        \"\"\"\n",
        "        self._cache_x = x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradients wrt input, weight, bias.\n",
        "        param grad_output: gradient wrt output\n",
        "        Return: (grad_input, grad_weight, grad_bias)\n",
        "        \"\"\"\n",
        "        x = self._cache_x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Convolution Layer\n",
        "Implement a 2D convolution layer with basic operations"
      ],
      "metadata": {
        "id": "ReQxnt0Cny3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias   = nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.reset_parameters()\n",
        "        self._cache_x = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Kaiming Uniform (fixed)\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Compute conv output using F.unfold and a batched dot-product.\n",
        "        Hints:\n",
        "          • Unfold to shape (N, C_in*k*k, L) where L is #locations.\n",
        "          • Reshape weights to (C_out, C_in*k*k).\n",
        "          • Add bias per output channel.\n",
        "          • Reshape to (N, C_out, H_out, W_out).\n",
        "        \"\"\"\n",
        "        self._cache_x = x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradients wrt input, weight, bias.\n",
        "        Hints:\n",
        "          • Reuse unfolded patches.\n",
        "          • grad_weight: correlate grad_output with unfolded input patches.\n",
        "          • grad_bias: sum grad_output over batch and spatial locations.\n",
        "          • grad_input: map grads back via an unfolded representation and fold.\n",
        "        Return: (grad_input, grad_weight, grad_bias)\n",
        "        \"\"\"\n",
        "        x = self._cache_x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "uupdLI22n9J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Max Pooling Layer\n",
        "Implement a max pooling layer"
      ],
      "metadata": {
        "id": "3Ri_dFZ4onU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomMaxPool2D(nn.Module):\n",
        "    def __init__(self, kernel_size, stride=None, padding=0):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride if stride is not None else kernel_size\n",
        "        self.padding = padding\n",
        "        self._cache_shape = None\n",
        "        self._cache_indices = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Max over k*k windows.\n",
        "        Hints:\n",
        "          • Unfold to (N, C*k*k, L) then view as (N, C, k*k, L).\n",
        "          • Take max over the k*k dimension and cache argmax indices + shapes.\n",
        "          • Reshape to (N, C, H_out, W_out).\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Route grads to max positions only and fold back.\n",
        "        Hints:\n",
        "          • Create a zero tensor for unfolded grads and scatter using indices.\n",
        "          • View back to (N, C*k*k, L) and fold into (H, W).\n",
        "        Return: grad_input\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "mPtvD4Bioyhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Cross-Entropy Loss [2 points]\n",
        "Implement cross-entropy loss with numerical stability"
      ],
      "metadata": {
        "id": "m6oBIYGJpP8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self._cache = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        TODO: Compute numerically-stable cross-entropy.\n",
        "        Hints:\n",
        "          • Subtract max per row before exponentiation.\n",
        "          • Convert to probabilities; pick class probs at targets.\n",
        "          • Apply -log(...) and reduction (mean by default).\n",
        "          • Cache what's needed for backward.\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, logits=None, targets=None):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradient w.r.t. logits.\n",
        "        Hints:\n",
        "          • If `logits` and `targets` are provided, recompute softmax probs stably.\n",
        "            Otherwise, use cached probs/targets from forward().\n",
        "          • Divide by batch size if reduction == 'mean'.\n",
        "        Return: grad_logits\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRiV60nKpU1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 2: Training the CNN\n",
        "\n",
        "Build and train the CNN for MNIST using **your custom layers**.\n",
        "\n",
        "### (a) Training pipeline code\n",
        "Address `TODO` in:\n",
        "1. **Backward pass section** — explicit chaining with your `backward()` methods.  \n",
        "2. **Parameter updates section** — manual updates using SGD-style with `torch.no_grad()`.\n",
        "\n",
        "### (b) Training and experiments\n",
        "Train your CNN on MNIST and run the following configurations:\n",
        "1. **Activations (2):** ReLU, Tanh.  \n",
        "2. **Dropout (2):** With (0.45/0.35), Without.\n",
        "\n",
        "Use consistent hyperparameters across runs. This yields 4 configurations.\n"
      ],
      "metadata": {
        "id": "3FCQy3mhqz2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_activation(name):\n",
        "    if name.lower() == \"relu\":\n",
        "        return F.relu, lambda z: (z > 0).to(z.dtype)\n",
        "    elif name.lower() == \"tanh\":\n",
        "        return torch.tanh, lambda z: 1 - torch.tanh(z)**2\n",
        "    else:\n",
        "        raise ValueError(\"Unknown activation: \" + name)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation_name=\"relu\", use_dropout=True, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.ACT, self.ACT_PRIME = get_activation(activation_name)\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "        self.conv1 = CustomConv2D(1, 32, 5)\n",
        "        self.conv2 = CustomConv2D(32, 64, 5)\n",
        "        self.max1  = CustomMaxPool2D(2, stride=2)\n",
        "        self.max2  = CustomMaxPool2D(2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.45) if use_dropout else nn.Identity()\n",
        "        self.dropout2 = nn.Dropout(0.35) if use_dropout else nn.Identity()\n",
        "        self.fc1   = CustomLinear(64*4*4, 512)\n",
        "        self.fc2   = CustomLinear(512, num_classes)\n",
        "        self.criterion = CustomCrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Return logits plus required caches (pre-activations) for manual backward\n",
        "        z1 = self.conv1(x);   a1 = self.ACT(z1); p1 = self.max1(a1); d1 = self.dropout1(p1)\n",
        "        z2 = self.conv2(d1);  a2 = self.ACT(z2); p2 = self.max2(a2); d2 = self.dropout2(p2)\n",
        "        flat = torch.flatten(d2, start_dim=1)\n",
        "        z3 = self.fc1(flat);  a3 = self.ACT(z3)\n",
        "        logits = self.fc2(a3)\n",
        "        caches = (z1, a1, p1, d1, z2, a2, p2, d2, z3, a3)\n",
        "        return logits, caches\n"
      ],
      "metadata": {
        "id": "zfmBUpKtrMj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data (MNIST)"
      ],
      "metadata": {
        "id": "7Ez96avjrVBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "full_train = datasets.MNIST(root=\"./data_CNN\", train=True, download=True, transform=ToTensor())\n",
        "test_ds    = datasets.MNIST(root=\"./data_CNN\", train=False, download=True, transform=ToTensor())\n",
        "\n",
        "train_size = int((1 - validation_split) * len(full_train))\n",
        "val_size   = len(full_train) - train_size\n",
        "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GvbsKSkyrbjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "n7eiVp0Pr-j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def accuracy_from_logits(logits, labels):\n",
        "    return (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "def sgd_update_(param, grad, lr):\n",
        "    param -= lr * grad\n",
        "    return param\n"
      ],
      "metadata": {
        "id": "iVCLP9mpsAuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Training Pipeline code\n",
        "Address the TODOs in the code below\n",
        "> - Complete the BACKWARD PASS (explicit chaining with your `.backward` methods).\n",
        "> - Uncomment the PARAMETER UPDATES (manual SGD with no_grad)\n"
      ],
      "metadata": {
        "id": "R7gkJ1mVsJbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_one_config(activation_name=\"relu\", use_dropout=True, epochs=num_epochs, lr=learning_rate, num_classes=num_classes):\n",
        "    model = CNN(activation_name=activation_name, use_dropout=use_dropout, num_classes=num_classes).to(device)\n",
        "    criterion = model.criterion\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs,   val_accs   = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss, total_train_correct, total_train_examples = 0.0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            # ---------- Forward ----------\n",
        "            logits, caches = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            # Unpack caches (pre/post activations)\n",
        "            z1, a1, p1, d1, z2, a2, p2, d2, z3, a3 = caches\n",
        "\n",
        "            # ==========================================================================\n",
        "            # BACKWARD PASS (explicit chaining with your .backward methods)\n",
        "            # TODO:\n",
        "            #   1- Check PARAMETER UPDATES codes below for ideas and some variable names\n",
        "            #   2- Chain in the following way:\n",
        "            #       - dL/dlogits from Cross-Entropy Loss\n",
        "            #       - FC2\n",
        "            #       - Activation after FC1\n",
        "            #       - FC1\n",
        "            #       - Reshape to feature map\n",
        "            #       - Dropout2 backward (inverted dropout approximation by keep-prob)\n",
        "            #       - MaxPool2 backward\n",
        "            #       - Activation after Conv2\n",
        "            #       - Conv2 backward\n",
        "            #       - Dropout1 backward\n",
        "            #       - MaxPool1 backward\n",
        "            #       - Activation after Conv1\n",
        "            #       - Conv1 backward\n",
        "            \"WRITE YOUR CODE HERE\"\n",
        "\n",
        "\n",
        "            # ===========================================\n",
        "            # PARAMETER UPDATES (manual SGD with no_grad)\n",
        "            # TODO: UNCOMMENT THE CODES BELOW, DO NOT CHANGE\n",
        "            # ===========================================\n",
        "            # with torch.no_grad():\n",
        "            #     # FC2\n",
        "            #     model.fc2.weight.copy_(sgd_update_(model.fc2.weight, grad_fc2_w, lr))\n",
        "            #     if model.fc2.bias is not None:\n",
        "            #         model.fc2.bias.copy_(sgd_update_(model.fc2.bias, grad_fc2_b, lr))\n",
        "            #     # FC1\n",
        "            #     model.fc1.weight.copy_(sgd_update_(model.fc1.weight, grad_fc1_w, lr))\n",
        "            #     if model.fc1.bias is not None:\n",
        "            #         model.fc1.bias.copy_(sgd_update_(model.fc1.bias, grad_fc1_b, lr))\n",
        "            #     # Conv2\n",
        "            #     model.conv2.weight.copy_(sgd_update_(model.conv2.weight, grad_conv2_w, lr))\n",
        "            #     model.conv2.bias.copy_(sgd_update_(model.conv2.bias, grad_conv2_b, lr))\n",
        "            #     # Conv1\n",
        "            #     model.conv1.weight.copy_(sgd_update_(model.conv1.weight, grad_conv1_w, lr))\n",
        "            #     model.conv1.bias.copy_(sgd_update_(model.conv1.bias, grad_conv1_b, lr))\n",
        "\n",
        "\n",
        "            # ---------- Train metrics ----------\n",
        "            with torch.no_grad():\n",
        "                total_train_loss += loss.item()\n",
        "                total_train_examples += yb.size(0)\n",
        "                total_train_correct += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        train_losses.append(total_train_loss / len(train_loader))\n",
        "        train_accs.append(total_train_correct / total_train_examples)\n",
        "\n",
        "        # ---------- Validation ----------\n",
        "        model.eval()\n",
        "        val_loss_sum, val_correct, val_examples = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits, _ = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_loss_sum += loss.item()\n",
        "                val_examples += yb.size(0)\n",
        "                val_correct += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss_sum / len(val_loader))\n",
        "        val_accs.append(val_correct / val_examples)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d} | \"\n",
        "              f\"Train Loss {train_losses[-1]:.4f}  Acc {train_accs[-1]*100:.2f}% | \"\n",
        "              f\"Val Loss {val_losses[-1]:.4f}  Acc {val_accs[-1]*100:.2f}%\")\n",
        "\n",
        "    # ---------- Test ----------\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits, _ = model(xb)\n",
        "            test_correct += (logits.argmax(1) == yb).sum().item()\n",
        "            test_total += yb.size(0)\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses, \"val_loss\": val_losses,\n",
        "        \"train_acc\": train_accs,   \"val_acc\": val_accs,\n",
        "        \"test_acc\": test_acc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "p3oQ2mxPtT8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training, and experiments\n",
        "Train your CNN on MNIST and run the following configurations:\n",
        "1. **Activations (2):** ReLU, Tanh.  \n",
        "2. **Dropout (2):** With (0.45/0.35), Without.\n",
        "\n",
        "Use consistent hyperparameters across runs. This yields 4 configurations:\n",
        "  - ReLU / With Dropout\n",
        "  - ReLU / Without Dropout\n",
        "  - Tanh / With Dropout\n",
        "  - Tanh / Without Dropout"
      ],
      "metadata": {
        "id": "N5cGWM7Rv1X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training, and experiments : 4 configurations in total\n",
        "\"WRITE YOUR CODE HERE\""
      ],
      "metadata": {
        "id": "10i1TyZowgSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 3: Results and Discussions\n",
        "\n",
        "1. **Plots:** Submit clear plots of training and validation **accuracy** and **loss** for each configuration. In addition, include the **test-set accuracy** on these plots.\n"
      ],
      "metadata": {
        "id": "gqKoezEJxIXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots\n",
        "\"WRITE YOUR CODE HERE\""
      ],
      "metadata": {
        "id": "bSMcfgOqx-qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  \n",
        "2. **Write a short discussion:** (~1 page) reflecting on:\n",
        "   - Which activations/dropout choices performed best and why?\n",
        "   - Any signs of over/underfitting you observed.\n",
        "\n",
        "   Note: Upload this discussion as a pdf file on Gradescope"
      ],
      "metadata": {
        "id": "yj3W2K4yxVtY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZiBviemUx4Kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}